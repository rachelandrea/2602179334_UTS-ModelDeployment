# -*- coding: utf-8 -*-
"""UTS_Model_Deployment_2602179334_Rachel_Andrea_Sumaiku.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uib3Gke6vON0lQJcAuP8CdfzQ2i4hTjs

#2. Mengubah proses training model terbaik yang telah diperoleh dalam pemodelan di atas dalam format OOP. Model terbaik yang didapat: XGBoost.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
from sklearn.model_selection import train_test_split
import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score

# Function for exploratory data analysis and feature engineering
class DataProcessing:
  def __init__(self, file_path):
    self.file_path = file_path
    self.data = None
    self.input_df = None
    self.output_df = None

  #EDA
  def load_data(self):
    self.data = pd.read_csv(self.file_path)

  def dataInfo(self):
    self.data.info()

  def checkUniqueValue(self):
    for c in self.data:
      print(f'{c}: {self.data[c].nunique()}')
      print(self.data[c].unique())
      print()

  def checkNullValue(self):
    print(self.data.isnull().sum())

  def checkOutlier(self):
    for col in self.data:
      plt.boxplot(self.data[col])
      plt.show()
      print(col)
    # boxplot = self.data.boxplot(column=[kolom])
    # plt.show()

  def checkDuplicate(self):
    print(self.data.duplicated().sum())

  # Feature engineering
  def meanValue(self, kolom):
    return np.mean(self.data[kolom])

  def fillNAValue(self, kolom, num):
    self.data[kolom].fillna(num, inplace=True)

  def labelEncoding(self, kolom):
    from sklearn.preprocessing import LabelEncoder

    labelEnc = LabelEncoder()
    self.data[kolom] = labelEnc.fit_transform(self.data[kolom])

  def dataConvertToNumeric(self, kolom):
    self.data[kolom] = pd.to_numeric(self.data[kolom], errors='coerce')

  def drop_column(self, kolom):
    self.data.drop(columns=kolom, axis=1, inplace=True)

  def removeOutlier(self, kolom):
    mean = self.data[kolom].mean()
    std = self.data[kolom].std()
    Tmax = mean + (3 * std)
    Tmin = mean - (3 * std)
    #store outlier value
    Outlier = self.data[(self.data[kolom] < Tmin) | (self.data[kolom] > Tmax)][kolom].values
    #remove outlier
    self.data = self.data[~self.data[kolom].isin(Outlier)]

  # Create input output
  def create_InputOutput(self, target):
    self.output_df = self.data[target]
    self.input_df = self.data.drop(target, axis=1)

#Algoritma XGboost
class Modelling:
    def __init__(self, input_data, output_data):
        self.input_data = input_data
        self.output_data = output_data
        self.createModel()
        self.x_train, self.x_test, self.y_train, self.y_test, self.y_predict = [None] * 5

    def split_data(self, test_size=0.2, random_state=42):
        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.input_data, self.output_data, test_size=test_size, random_state=random_state)

    def createModel(self,objective='binary:logistic', random_state=42, colsample_bytree=1.0, gamma=0, learning_rate=0.1, max_depth=3, min_child_weight=1, subsample=1.0):
         self.model = XGBClassifier(objective=objective,
                                    random_state=random_state,
                                    colsample_bytree=colsample_bytree,
                                    gamma=gamma,
                                    learning_rate=learning_rate,
                                    max_depth=max_depth,
                                    min_child_weight=min_child_weight,
                                    subsample=subsample)

    def train_model(self):
         self.model.fit(self.x_train, self.y_train)

    def makePrediction(self):
        self.y_predict = self.model.predict(self.x_test)

    def createReport(self):
        print('\nClassification Report\n')
        print(classification_report(self.y_test, self.y_predict, target_names=['0','1']))

    def evaluate_model(self):
        predictions = self.model.predict(self.x_test)
        return accuracy_score(self.y_test, predictions)

    # Tuning parameter
    def tuningParameter(self):
        parameters = {
                      'max_depth': [3, 6, 10],
                      'min_child_weight': [1, 5, 10],
                      'gamma': [0.5, 1, 1.5, 2],
                      'subsample': [0.6, 0.8, 1.0],
                      'colsample_bytree': [0.6, 0.8, 1.0],
                      'learning_rate': [0.01, 0.1, 0.2]
                      }
        XGBoost_class = XGBClassifier()
        XGBoost_class = GridSearchCV(XGBoost_class,
                          parameters, #hyperparameter
                          scoring='accuracy', # matrix for scoring
                          cv=3)
        XGBoost_class.fit(self.x_train,self.y_train)
        print("Tuned Hyperparameters :", XGBoost_class.best_params_)
        print("Accuracy :",XGBoost_class.best_score_)
        self.createModel(colsample_bytree=XGBoost_class.best_params_['colsample_bytree'], gamma=XGBoost_class.best_params_['gamma'], learning_rate=XGBoost_class.best_params_['learning_rate'], max_depth=XGBoost_class.best_params_['max_depth'], min_child_weight=XGBoost_class.best_params_['min_child_weight'], subsample=XGBoost_class.best_params_['subsample'])

    def save_model_to_file(self, filename):
        with open(filename, 'wb') as file:  # Open the file in write-binary mode
            pickle.dump(self.model, file)  # Use pickle to write the model to the file

"""## Exploratory Data Analysis"""

data = 'data_D.csv'

data_processor = DataProcessing(data)
data_processor.load_data()
data_processor.dataInfo()

data_processor.data

data_processor.checkUniqueValue()

data_processor.checkNullValue()

data_processor.checkDuplicate()

"""## Feature Engineering"""

# Fill the Null Value in CreditScore with its mean
mean_CreditScore = data_processor.meanValue('CreditScore')
data_processor.fillNAValue('CreditScore',mean_CreditScore)

data_processor.checkNullValue()

# Drop the 'Unnamed: 0' and 'Surname' column
data_processor.drop_column('Unnamed: 0')
data_processor.drop_column('Surname')

# Encoding for object feature
data_processor.labelEncoding('Geography')
data_processor.labelEncoding('Gender')

data_processor.dataInfo()

data_processor.checkOutlier()

# Remove Outlier in feature CreditScore and Age
data_processor.removeOutlier('CreditScore')
data_processor.removeOutlier('Age')

data_processor.dataInfo()

"""## Modelling

### Splitting data into training and testing
"""

data_processor.create_InputOutput('churn')
input_df = data_processor.input_df
output_df = data_processor.output_df

XGBoost_model = Modelling(input_df, output_df)
XGBoost_model.split_data()

XGBoost_model.input_data

XGBoost_model.output_data

"""### Train the model"""

XGBoost_model.train_model()

XGBoost_model.makePrediction()
XGBoost_model.createReport()

"""### Tuning parameters"""

XGBoost_model.tuningParameter()
XGBoost_model.train_model()

XGBoost_model.makePrediction()
XGBoost_model.createReport()

XGBoost_model.save_model_to_file('best_model.pkl')